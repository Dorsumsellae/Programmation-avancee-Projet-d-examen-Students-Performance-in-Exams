{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2b31ee",
   "metadata": {},
   "source": [
    "# 05 - Hyperparameter Tuning (classification)\n",
    "\n",
    "Objectif : optimiser les modèles de classification (priorité rappel de la classe \"fail\") via GridSearchCV et RandomizedSearchCV, comparer aux baselines et sauvegarder le meilleur modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31cb38",
   "metadata": {},
   "source": [
    "## 1. Charger et préparer les données\n",
    "- Chargement `data/processed/train.csv` et `test.csv`\n",
    "- Drop des colonnes d'index sauvegardées\n",
    "- Définition des tâches (language, math, exam) avec features catégorielles\n",
    "- Préprocessing : OneHotEncoder pour toutes les features cat (éviter l'ordinalité)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a409b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, recall_score, f1_score, accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "RANDOM_STATE = 42\n",
    "CV_SPLITS = 5\n",
    "\n",
    "# Paths\n",
    "train_path = Path('..') / 'data' / 'processed' / 'train.csv'\n",
    "test_path = Path('..') / 'data' / 'processed' / 'test.csv'\n",
    "\n",
    "for p in [train_path, test_path]:\n",
    "    assert p.exists(), f\"Missing file: {p}\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Drop stray index columns\n",
    "cols_to_drop = [c for c in train_df.columns if c.startswith('Unnamed') or c == '']\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "test_df = test_df.drop(columns=cols_to_drop)\n",
    "\n",
    "base_cols = [\n",
    "    \"gender\",\n",
    "    \"race/ethnicity\",\n",
    "    \"parental level of education\",\n",
    "    \"lunch\",\n",
    "    \"test preparation course\",\n",
    "]\n",
    "\n",
    "tasks = {\n",
    "    \"language\": {\"target\": \"language passed\", \"features\": base_cols},\n",
    "    \"math\": {\"target\": \"math passed\", \"features\": base_cols},\n",
    "    \"exam\": {\"target\": \"exam passed\", \"features\": base_cols},\n",
    "}\n",
    "\n",
    "# Build splits per task (no split train/val here because CV handles it)\n",
    "data_splits = {}\n",
    "for name, spec in tasks.items():\n",
    "    X_train = train_df[spec[\"features\"]]\n",
    "    y_train = train_df[spec[\"target\"]]\n",
    "    X_test = test_df[spec[\"features\"]]\n",
    "    y_test = test_df[spec[\"target\"]]\n",
    "    data_splits[name] = {\"X_train\": X_train, \"y_train\": y_train, \"X_test\": X_test, \"y_test\": y_test}\n",
    "\n",
    "categorical_cols = base_cols\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "recall_fail_scorer = make_scorer(recall_score, pos_label=0)\n",
    "scoring = {\n",
    "    \"recall_fail\": recall_fail_scorer,\n",
    "    \"f1\": \"f1\",\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f556e5f",
   "metadata": {},
   "source": [
    "## 2. Définir le modèle de base\n",
    "Baseline : pipelines simples (LogReg, RF, GBoost, XGB, LGBM, Linear SVM calibré) avec pondération de classe. Mesure principale : recall_fail (classe 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b239d8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>recall_fail</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>language</td>\n",
       "      <td>log_reg</td>\n",
       "      <td>0.634662</td>\n",
       "      <td>0.779990</td>\n",
       "      <td>0.70000</td>\n",
       "      <td>0.755340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>language</td>\n",
       "      <td>xgb</td>\n",
       "      <td>0.601772</td>\n",
       "      <td>0.759311</td>\n",
       "      <td>0.67375</td>\n",
       "      <td>0.713811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>language</td>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.592691</td>\n",
       "      <td>0.740367</td>\n",
       "      <td>0.65250</td>\n",
       "      <td>0.693787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>language</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.516611</td>\n",
       "      <td>0.756828</td>\n",
       "      <td>0.66250</td>\n",
       "      <td>0.671325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>language</td>\n",
       "      <td>gboost</td>\n",
       "      <td>0.307752</td>\n",
       "      <td>0.835977</td>\n",
       "      <td>0.74125</td>\n",
       "      <td>0.720681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task    model  recall_fail        f1  accuracy   roc_auc\n",
       "0  language  log_reg     0.634662  0.779990   0.70000  0.755340\n",
       "3  language      xgb     0.601772  0.759311   0.67375  0.713811\n",
       "4  language     lgbm     0.592691  0.740367   0.65250  0.693787\n",
       "1  language       rf     0.516611  0.756828   0.66250  0.671325\n",
       "2  language   gboost     0.307752  0.835977   0.74125  0.720681"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baselines (option: skip exam if pretrained models are available)\n",
    "USE_PRETRAINED_EXAM = True\n",
    "\n",
    "\n",
    "def make_models_for_task(y_train):\n",
    "    n_fail = max((y_train == 0).sum(), 1)\n",
    "    n_pass = max((y_train == 1).sum(), 1)\n",
    "    fail_weight = n_pass / n_fail\n",
    "    scale_pos_weight = n_fail / n_pass\n",
    "\n",
    "    return {\n",
    "        \"log_reg\": Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", LogisticRegression(max_iter=1000, n_jobs=-1, class_weight=\"balanced\", solver=\"lbfgs\")),\n",
    "        ]),\n",
    "        \"rf\": Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", RandomForestClassifier(n_estimators=400, random_state=RANDOM_STATE, n_jobs=-1, class_weight=\"balanced\")),\n",
    "        ]),\n",
    "        \"gboost\": Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", GradientBoostingClassifier(random_state=RANDOM_STATE)),\n",
    "        ]),\n",
    "        \"xgb\": Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", XGBClassifier(\n",
    "                n_estimators=400,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                eval_metric=\"logloss\",\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "            )),\n",
    "        ]),\n",
    "        \"lgbm\": Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", LGBMClassifier(\n",
    "                n_estimators=400,\n",
    "                max_depth=-1,\n",
    "                num_leaves=31,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                random_state=RANDOM_STATE,\n",
    "                class_weight={0: fail_weight, 1: 1.0},\n",
    "            )),\n",
    "        ]),\n",
    "        \"linear_svm\": Pipeline([\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", CalibratedClassifierCV(\n",
    "                estimator=LinearSVC(class_weight=\"balanced\"),\n",
    "                cv=3,\n",
    "                n_jobs=-1,\n",
    "            )),\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "\n",
    "def cv_eval(model, X, y):\n",
    "    cv_res = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "    return {k.replace(\"test_\", \"\"): np.mean(v) for k, v in cv_res.items() if k.startswith(\"test_\")}\n",
    "\n",
    "# Baseline CV per task (skip exam if using pretrained)\n",
    "baseline_rows = []\n",
    "baseline_results = {}\n",
    "models_by_task = {}\n",
    "for task, split in data_splits.items():\n",
    "    if USE_PRETRAINED_EXAM and task == \"exam\":\n",
    "        continue\n",
    "    models = make_models_for_task(split[\"y_train\"])\n",
    "    models_by_task[task] = models\n",
    "    baseline_results[task] = {}\n",
    "    for name, model in models.items():\n",
    "        res = cv_eval(model, split[\"X_train\"], split[\"y_train\"])\n",
    "        baseline_results[task][name] = res\n",
    "        baseline_rows.append({\"task\": task, \"model\": name, **res})\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_rows).sort_values([\"task\", \"recall_fail\"], ascending=[True, False])\n",
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea9043",
   "metadata": {},
   "source": [
    "## 3. GridSearchCV (espace restreint, explicite)\n",
    "- Modèle ciblé : XGBClassifier\n",
    "- Justification : profondeur modérée, lr faible, subsample/colsample pour limiter l’overfit, scale_pos_weight dérivé du ratio classes.\n",
    "- Param_grid (exemple) :\n",
    "  - max_depth: [3, 4, 5]\n",
    "  - learning_rate: [0.02, 0.05, 0.1]\n",
    "  - n_estimators: [200, 400, 600]\n",
    "  - subsample: [0.7, 0.9]\n",
    "  - colsample_bytree: [0.7, 0.9]\n",
    "  - scale_pos_weight: [ratio, ratio*0.7, ratio*1.3] (ratio = n_fail/n_pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd172c9",
   "metadata": {},
   "source": [
    "## 3.1 Utiliser les modèles exam déjà entraînés (04b)\n",
    "Si disponibles dans `models/exam_*.joblib`, on les charge pour éviter de réentraîner la baseline exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39505c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:131: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Le fichier spécifié est introuvable\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 247, in _count_physical_cores\n",
      "    cpu_count_physical = _count_physical_cores_win32()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 299, in _count_physical_cores_win32\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall_fail</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exam</td>\n",
       "      <td>log_reg</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.672065</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.682821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exam</td>\n",
       "      <td>xgb</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.646583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exam</td>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.674603</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.629655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   task    model  accuracy        f1  recall_fail   roc_auc\n",
       "1  exam  log_reg     0.595  0.672065     0.654545  0.682821\n",
       "2  exam      xgb     0.615  0.698039     0.618182  0.646583\n",
       "0  exam     lgbm     0.590  0.674603     0.600000  0.629655"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger les modèles exam déjà entraînés (si présents dans models/exam_*.joblib)\n",
    "exam_split = data_splits[\"exam\"]\n",
    "pretrained_exam_rows = []\n",
    "models_dir = Path(\"..\") / \"models\" / \"classification\"\n",
    "if USE_PRETRAINED_EXAM and models_dir.exists():\n",
    "    for path in models_dir.glob(\"exam_*.joblib\"):\n",
    "        model_name = path.stem.replace(\"exam_\", \"\")\n",
    "        model = joblib.load(path)\n",
    "        preds = model.predict(exam_split[\"X_test\"])\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(exam_split[\"X_test\"])[:, 1]\n",
    "            auc = roc_auc_score(exam_split[\"y_test\"], proba)\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            proba = model.decision_function(exam_split[\"X_test\"])\n",
    "            auc = roc_auc_score(exam_split[\"y_test\"], proba)\n",
    "        else:\n",
    "            proba = None\n",
    "            auc = np.nan\n",
    "        pretrained_exam_rows.append(\n",
    "            {\n",
    "                \"task\": \"exam\",\n",
    "                \"model\": model_name,\n",
    "                \"accuracy\": accuracy_score(exam_split[\"y_test\"], preds),\n",
    "                \"f1\": f1_score(exam_split[\"y_test\"], preds),\n",
    "                \"recall_fail\": recall_score(exam_split[\"y_test\"], preds, pos_label=0),\n",
    "                \"roc_auc\": auc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "pretrained_exam_df = pd.DataFrame(pretrained_exam_rows).sort_values(\"recall_fail\", ascending=False)\n",
    "pretrained_exam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93acb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [13:46:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>recall_fail</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "      <th>param_subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgb_grid</td>\n",
       "      <td>0.807879</td>\n",
       "      <td>0.616588</td>\n",
       "      <td>0.57125</td>\n",
       "      <td>0.710984</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.272222</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  recall_fail        f1  accuracy   roc_auc  \\\n",
       "0  xgb_grid     0.807879  0.616588   0.57125  0.710984   \n",
       "\n",
       "   param_colsample_bytree  param_learning_rate  param_max_depth  \\\n",
       "0                     0.9                 0.02                3   \n",
       "\n",
       "   param_n_estimators  param_scale_pos_weight  param_subsample  \n",
       "0                 200                0.272222              0.9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search XGB (task exam)\n",
    "exam_split = data_splits[\"exam\"]\n",
    "X_exam, y_exam = exam_split[\"X_train\"], exam_split[\"y_train\"]\n",
    "n_fail = max((y_exam == 0).sum(), 1)\n",
    "n_pass = max((y_exam == 1).sum(), 1)\n",
    "fail_pass_ratio = n_fail / n_pass\n",
    "\n",
    "xgb_base = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\n",
    "        \"clf\",\n",
    "        XGBClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            eval_metric=\"logloss\",\n",
    "            n_jobs=-1,\n",
    "            use_label_encoder=False,\n",
    "            scale_pos_weight=fail_pass_ratio,\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"clf__max_depth\": [3, 4, 5],\n",
    "    \"clf__learning_rate\": [0.02, 0.05, 0.1],\n",
    "    \"clf__n_estimators\": [200, 400, 600],\n",
    "    \"clf__subsample\": [0.7, 0.9],\n",
    "    \"clf__colsample_bytree\": [0.7, 0.9],\n",
    "    \"clf__scale_pos_weight\": [fail_pass_ratio * f for f in [0.7, 1.0, 1.3]],\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb_base,\n",
    "    xgb_param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"recall_fail\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_exam, y_exam)\n",
    "\n",
    "def extract_best_scores(search):\n",
    "    idx = search.best_index_\n",
    "    res = search.cv_results_\n",
    "    return {\n",
    "        \"recall_fail\": res[\"mean_test_recall_fail\"][idx],\n",
    "        \"f1\": res[\"mean_test_f1\"][idx],\n",
    "        \"accuracy\": res[\"mean_test_accuracy\"][idx],\n",
    "        \"roc_auc\": res[\"mean_test_roc_auc\"][idx],\n",
    "    }\n",
    "\n",
    "xgb_grid_scores = extract_best_scores(grid_xgb)\n",
    "pd.DataFrame([\n",
    "    {\"model\": \"xgb_grid\", **xgb_grid_scores, **{k.replace(\"clf__\", \"param_\"): v for k, v in grid_xgb.best_params_.items()}},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58bb9c",
   "metadata": {},
   "source": [
    "## 4. RandomizedSearchCV (espace plus large)\n",
    "- Modèle ciblé : LGBMClassifier\n",
    "- Justification : balayage plus large via distributions continues pour lr/subsample/colsample et discrètes pour num_leaves/n_estimators.\n",
    "- n_iter modéré pour rester raisonnable en temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af55c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[LightGBM] [Info] Number of positive: 576, number of negative: 224\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 34\n",
      "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>recall_fail</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_num_leaves</th>\n",
       "      <th>param_subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lgbm_random</td>\n",
       "      <td>0.598485</td>\n",
       "      <td>0.729212</td>\n",
       "      <td>0.64625</td>\n",
       "      <td>0.688556</td>\n",
       "      <td>0.920879</td>\n",
       "      <td>0.031183</td>\n",
       "      <td>6</td>\n",
       "      <td>208</td>\n",
       "      <td>38</td>\n",
       "      <td>0.757953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  recall_fail        f1  accuracy   roc_auc  \\\n",
       "0  lgbm_random     0.598485  0.729212   0.64625  0.688556   \n",
       "\n",
       "   param_colsample_bytree  param_learning_rate  param_max_depth  \\\n",
       "0                0.920879             0.031183                6   \n",
       "\n",
       "   param_n_estimators  param_num_leaves  param_subsample  \n",
       "0                 208                38         0.757953  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomized search LGBM (task exam)\n",
    "fail_weight = n_pass / n_fail\n",
    "\n",
    "lgbm_base = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\n",
    "        \"clf\",\n",
    "        LGBMClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight={0: fail_weight, 1: 1.0},\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "\n",
    "lgbm_param_dist = {\n",
    "    \"clf__num_leaves\": randint(15, 64),\n",
    "    \"clf__max_depth\": [-1, 4, 6, 8],\n",
    "    \"clf__learning_rate\": uniform(0.02, 0.15),\n",
    "    \"clf__n_estimators\": randint(200, 601),\n",
    "    \"clf__subsample\": uniform(0.6, 0.4),\n",
    "    \"clf__colsample_bytree\": uniform(0.6, 0.4),\n",
    "}\n",
    "\n",
    "rand_lgbm = RandomizedSearchCV(\n",
    "    lgbm_base,\n",
    "    lgbm_param_dist,\n",
    "    n_iter=25,\n",
    "    scoring=scoring,\n",
    "    refit=\"recall_fail\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "rand_lgbm.fit(X_exam, y_exam)\n",
    "\n",
    "lgbm_rand_scores = extract_best_scores(rand_lgbm)\n",
    "pd.DataFrame([\n",
    "    {\"model\": \"lgbm_random\", **lgbm_rand_scores, **{k.replace(\"clf__\", \"param_\"): v for k, v in rand_lgbm.best_params_.items()}},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a57057",
   "metadata": {},
   "source": [
    "## 5. Comparaison baselines vs modèles tunés (CV)\n",
    "- Agrégation des scores CV (recall_fail prioritaire).\n",
    "- Sélection du meilleur modèle tuned sur la tâche exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7ddc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall_fail</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exam</td>\n",
       "      <td>xgb_grid</td>\n",
       "      <td>0.57125</td>\n",
       "      <td>0.616588</td>\n",
       "      <td>0.807879</td>\n",
       "      <td>0.710984</td>\n",
       "      <td>tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exam</td>\n",
       "      <td>log_reg</td>\n",
       "      <td>0.59500</td>\n",
       "      <td>0.672065</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.682821</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exam</td>\n",
       "      <td>xgb</td>\n",
       "      <td>0.61500</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.646583</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exam</td>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.59000</td>\n",
       "      <td>0.674603</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.629655</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exam</td>\n",
       "      <td>lgbm_random</td>\n",
       "      <td>0.64625</td>\n",
       "      <td>0.729212</td>\n",
       "      <td>0.598485</td>\n",
       "      <td>0.688556</td>\n",
       "      <td>tuned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   task        model  accuracy        f1  recall_fail   roc_auc      type\n",
       "3  exam     xgb_grid   0.57125  0.616588     0.807879  0.710984     tuned\n",
       "0  exam      log_reg   0.59500  0.672065     0.654545  0.682821  baseline\n",
       "1  exam          xgb   0.61500  0.698039     0.618182  0.646583  baseline\n",
       "2  exam         lgbm   0.59000  0.674603     0.600000  0.629655  baseline\n",
       "4  exam  lgbm_random   0.64625  0.729212     0.598485  0.688556     tuned"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_search(name, search):\n",
    "    scores = extract_best_scores(search)\n",
    "    return {\"task\": \"exam\", \"model\": name, **scores}\n",
    "\n",
    "exam_baseline = (\n",
    "    pretrained_exam_df.copy()\n",
    "    if USE_PRETRAINED_EXAM and not pretrained_exam_df.empty\n",
    "    else baseline_df[baseline_df[\"task\"] == \"exam\"].copy()\n",
    ")\n",
    "\n",
    "# Si on a sauté l'exam baseline et qu'aucun modèle pré-entraîné n'est présent, prévenir\n",
    "if exam_baseline.empty:\n",
    "    print(\"Aucun baseline exam disponible : vérifiez models/exam_*.joblib ou désactivez USE_PRETRAINED_EXAM.\")\n",
    "\n",
    "\n",
    "tuned_rows = [\n",
    "    summarize_search(\"xgb_grid\", grid_xgb),\n",
    "    summarize_search(\"lgbm_random\", rand_lgbm),\n",
    "]\n",
    "tuned_df = pd.DataFrame(tuned_rows)\n",
    "\n",
    "comparison_df = pd.concat(\n",
    "    [exam_baseline.assign(type=\"baseline\"), tuned_df.assign(type=\"tuned\")],\n",
    "    ignore_index=True,\n",
    ")\n",
    "comparison_df.sort_values(\"recall_fail\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f5fa9",
   "metadata": {},
   "source": [
    "## 6. Évaluation sur test + sauvegarde du meilleur modèle\n",
    "- Sélection du meilleur tuned (rappel_fail CV).\n",
    "- Refit sur tout le train exam, évaluation sur test, sauvegarde `models/exam_tuned.joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b09d750b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tuned model: xgb_grid\n",
      "Model saved to ..\\models\\exam_tuned.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\anaconda3\\envs\\Data\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [13:47:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "search_map = {\n",
    "    \"xgb_grid\": grid_xgb,\n",
    "    \"lgbm_random\": rand_lgbm,\n",
    "}\n",
    "\n",
    "best_row = (\n",
    "    comparison_df[comparison_df[\"type\"] == \"tuned\"]\n",
    "    .sort_values(\"recall_fail\", ascending=False)\n",
    "    .iloc[0]\n",
    ")\n",
    "best_name = best_row[\"model\"]\n",
    "best_search = search_map[best_name]\n",
    "\n",
    "best_estimator = best_search.best_estimator_\n",
    "best_estimator.fit(X_exam, y_exam)\n",
    "\n",
    "X_test_exam, y_test_exam = exam_split[\"X_test\"], exam_split[\"y_test\"]\n",
    "test_pred = best_estimator.predict(X_test_exam)\n",
    "if hasattr(best_estimator, \"predict_proba\"):\n",
    "    test_proba = best_estimator.predict_proba(X_test_exam)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test_exam, test_proba)\n",
    "else:\n",
    "    test_proba = None\n",
    "    test_auc = np.nan\n",
    "\n",
    "test_metrics = {\n",
    "    \"recall_fail\": recall_score(y_test_exam, test_pred, pos_label=0),\n",
    "    \"f1\": f1_score(y_test_exam, test_pred),\n",
    "    \"accuracy\": accuracy_score(y_test_exam, test_pred),\n",
    "    \"roc_auc\": test_auc,\n",
    "}\n",
    "\n",
    "report = classification_report(y_test_exam, test_pred, target_names=[\"fail\", \"pass\"], output_dict=True)\n",
    "\n",
    "models_dir = Path(\"..\") / \"models\"\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_path = models_dir / \"exam_tuned.joblib\"\n",
    "joblib.dump(best_estimator, model_path)\n",
    "\n",
    "print(f\"Best tuned model: {best_name}\")\n",
    "pd.DataFrame([test_metrics])\n",
    "pd.DataFrame(report).T\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
