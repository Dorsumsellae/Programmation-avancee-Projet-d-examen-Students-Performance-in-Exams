{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Modélisation & Régression Complète\n",
    "\n",
    "Ce notebook entraîne plusieurs modèles de régression, effectue un tuning d'hyperparamètres, et sélectionne le meilleur modèle.\n",
    "\n",
    "## Structure\n",
    "1. Imports et configuration\n",
    "2. Chargement des données\n",
    "3. Entraînement des modèles baseline\n",
    "4. Validation croisée\n",
    "5. Tuning des hyperparamètres\n",
    "6. Évaluation et comparaison\n",
    "7. Analyse d'erreurs\n",
    "8. Feature importance\n",
    "9. Sauvegarde des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTS ET CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "# Configuration pour reproductibilité\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Style des graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Créer les dossiers s'ils n'existent pas\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports réussis\")\n",
    "print(f\"✓ Random state fixé à {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CHARGEMENT DES DONNÉES PRÉTRAITÉES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les données doivent être prétraitées (voir notebook 03)\n",
    "X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv').squeeze()\n",
    "y_test = pd.read_csv('data/processed/y_test.csv').squeeze()\n",
    "\n",
    "print(\"\\n=== DONNÉES CHARGÉES ===\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Vérification\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Min: {y_train.min():.2f}, Max: {y_train.max():.2f}\")\n",
    "print(f\"  Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ENTRAÎNEMENT DES MODÈLES BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ENTRAÎNEMENT DES MODÈLES BASELINE ===\\n\")\n",
    "\n",
    "models = {}\n",
    "baseline_results = {}\n",
    "training_times = {}\n",
    "\n",
    "# 3.1 Linear Regression\n",
    "print(\"1. Linear Regression...\")\n",
    "start_time = time.time()\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "models['Linear Regression'] = model_lr\n",
    "training_times['Linear Regression'] = time.time() - start_time\n",
    "print(f\"   ✓ Entraîné en {training_times['Linear Regression']:.4f}s\")\n",
    "\n",
    "# 3.2 Ridge Regression\n",
    "print(\"2. Ridge Regression (α=1.0)...\")\n",
    "start_time = time.time()\n",
    "model_ridge = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "model_ridge.fit(X_train, y_train)\n",
    "models['Ridge (α=1.0)'] = model_ridge\n",
    "training_times['Ridge (α=1.0)'] = time.time() - start_time\n",
    "print(f\"   ✓ Entraîné en {training_times['Ridge (α=1.0)']:.4f}s\")\n",
    "\n",
    "# 3.3 Lasso Regression\n",
    "print(\"3. Lasso Regression (α=0.1)...\")\n",
    "start_time = time.time()\n",
    "model_lasso = Lasso(alpha=0.1, random_state=RANDOM_STATE, max_iter=10000)\n",
    "model_lasso.fit(X_train, y_train)\n",
    "models['Lasso (α=0.1)'] = model_lasso\n",
    "training_times['Lasso (α=0.1)'] = time.time() - start_time\n",
    "print(f\"   ✓ Entraîné en {training_times['Lasso (α=0.1)']:.4f}s\")\n",
    "\n",
    "# 3.4 Random Forest Regressor\n",
    "print(\"4. Random Forest (100 arbres)...\")\n",
    "start_time = time.time()\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    max_depth=15\n",
    ")\n",
    "model_rf.fit(X_train, y_train)\n",
    "models['Random Forest'] = model_rf\n",
    "training_times['Random Forest'] = time.time() - start_time\n",
    "print(f\"   ✓ Entraîné en {training_times['Random Forest']:.4f}s\")\n",
    "\n",
    "# 3.5 Gradient Boosting Regressor\n",
    "print(\"5. Gradient Boosting Regressor...\")\n",
    "start_time = time.time()\n",
    "model_gb = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "model_gb.fit(X_train, y_train)\n",
    "models['Gradient Boosting'] = model_gb\n",
    "training_times['Gradient Boosting'] = time.time() - start_time\n",
    "print(f\"   ✓ Entraîné en {training_times['Gradient Boosting']:.4f}s\")\n",
    "\n",
    "print(\"\\n✓ Tous les modèles baseline sont entraînés!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VALIDATION CROISÉE (5-FOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== VALIDATION CROISÉE (5-FOLD) ===\\n\")\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Scores de CV\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train,\n",
    "        cv=kfold,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cv_results[model_name] = {\n",
    "        'mean_r2': cv_scores.mean(),\n",
    "        'std_r2': cv_scores.std(),\n",
    "        'scores': cv_scores.tolist()\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  R² CV: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "    print(f\"  Scores: {[f'{s:.4f}' for s in cv_scores]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ÉVALUATION BASELINE SUR LE JEUX DE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ÉVALUATION BASELINE (TEST SET) ===\\n\")\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Évalue un modèle et retourne les métriques.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Évaluer tous les modèles\n",
    "baseline_metrics = {}\n",
    "predictions_baseline = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    metrics = evaluate_model(model, X_test, y_test, model_name)\n",
    "    baseline_metrics[model_name] = metrics\n",
    "    predictions_baseline[model_name] = metrics.pop('y_pred')\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
    "    print(f\"  MAE:  {metrics['MAE']:.4f}\")\n",
    "    print(f\"  R²:   {metrics['R2']:.4f}\")\n",
    "    print(f\"  MAPE: {metrics['MAPE']:.4f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COMPARAISON DES MODÈLES BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== TABLEAU DE COMPARAISON DES MODÈLES ===\\n\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'RMSE': baseline_metrics[model_name]['RMSE'],\n",
    "        'MAE': baseline_metrics[model_name]['MAE'],\n",
    "        'R²': baseline_metrics[model_name]['R2'],\n",
    "        'MAPE': baseline_metrics[model_name]['MAPE'],\n",
    "        'CV_R²': cv_results[model_name]['mean_r2'],\n",
    "        'Training Time (s)': training_times[model_name]\n",
    "    }\n",
    "    for model_name in models.keys()\n",
    "}).T\n",
    "\n",
    "# Arrondir et trier par R²\n",
    "comparison_df = comparison_df.round(4)\n",
    "comparison_df_sorted = comparison_df.sort_values('R²', ascending=False)\n",
    "\n",
    "print(comparison_df_sorted)\n",
    "print(f\"\\n✓ Meilleur modèle baseline: {comparison_df_sorted.index[0]}\")\n",
    "\n",
    "# Sauvegarder le tableau\n",
    "comparison_df_sorted.to_csv('reports/baseline_comparison.csv')\n",
    "print(\"✓ Tableau sauvegardé dans reports/baseline_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. VISUALISATION - COMPARAISON DES MODÈLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "ax = axes[0]\n",
    "models_names = list(baseline_metrics.keys())\n",
    "rmse_values = [baseline_metrics[m]['RMSE'] for m in models_names]\n",
    "colors = ['#FF6B6B' if v == max(rmse_values) else '#4ECDC4' if v == min(rmse_values) else '#95E1D3' \n",
    "          for v in rmse_values]\n",
    "ax.barh(models_names, rmse_values, color=colors)\n",
    "ax.set_xlabel('RMSE (plus bas = mieux)')\n",
    "ax.set_title('Comparaison RMSE - Baseline')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# R² comparison\n",
    "ax = axes[1]\n",
    "r2_values = [baseline_metrics[m]['R2'] for m in models_names]\n",
    "colors = ['#4ECDC4' if v == max(r2_values) else '#FF6B6B' if v == min(r2_values) else '#95E1D3' \n",
    "          for v in r2_values]\n",
    "ax.barh(models_names, r2_values, color=colors)\n",
    "ax.set_xlabel('R² (plus haut = mieux)')\n",
    "ax.set_title('Comparaison R² - Baseline')\n",
    "ax.set_xlim(0.6, 0.8)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# MAE comparison\n",
    "ax = axes[2]\n",
    "mae_values = [baseline_metrics[m]['MAE'] for m in models_names]\n",
    "colors = ['#FF6B6B' if v == max(mae_values) else '#4ECDC4' if v == min(mae_values) else '#95E1D3' \n",
    "          for v in mae_values]\n",
    "ax.barh(models_names, mae_values, color=colors)\n",
    "ax.set_xlabel('MAE (plus bas = mieux)')\n",
    "ax.set_title('Comparaison MAE - Baseline')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/baseline_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Graphique sauvegardé dans reports/baseline_models_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TUNING D'HYPERPARAMÈTRES (GRADIENT BOOSTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== HYPERPARAMETER TUNING - GRADIENT BOOSTING ===\\n\")\n",
    "\n",
    "# Paramètres de base du meilleur modèle\n",
    "print(\"Modèle choisi pour le tuning: Gradient Boosting\")\n",
    "print(\"Raison: Meilleur R² et RMSE parmi les modèles baseline\\n\")\n",
    "\n",
    "# Espace de recherche\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 150, 200, 250, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"Espace de recherche:\")\n",
    "for param, values in param_dist.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# RandomizedSearchCV\n",
    "print(\"\\nLancement de RandomizedSearchCV (50 itérations)...\")\n",
    "print(\"Métrique: Negative RMSE (neg_root_mean_squared_error)\")\n",
    "print(\"CV: 5-fold\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(random_state=RANDOM_STATE),\n",
    "    param_dist,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "tuning_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Tuning complété en {tuning_time/60:.2f} minutes\")\n",
    "\n",
    "# Résultats du tuning\n",
    "print(f\"\\nMeilleur RMSE (CV): {-random_search.best_score_:.4f}\")\n",
    "print(f\"Meilleur R² (CV): {random_search.best_estimator_.score(X_train, y_train):.4f}\")\n",
    "\n",
    "print(\"\\nMeilleurs hyperparamètres:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ÉVALUATION DU MODÈLE TUNÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ÉVALUATION DU MODÈLE TUNÉ ===\\n\")\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "tuned_metrics = evaluate_model(best_model, X_test, y_test, \"Gradient Boosting (tuned)\")\n",
    "\n",
    "print(f\"Gradient Boosting (tuned):\")\n",
    "print(f\"  RMSE: {tuned_metrics['RMSE']:.4f}\")\n",
    "print(f\"  MAE:  {tuned_metrics['MAE']:.4f}\")\n",
    "print(f\"  R²:   {tuned_metrics['R2']:.4f}\")\n",
    "print(f\"  MAPE: {tuned_metrics['MAPE']:.4f}%\")\n",
    "\n",
    "# Comparaison baseline vs tuned\n",
    "baseline_gb_r2 = baseline_metrics['Gradient Boosting']['R2']\n",
    "tuned_gb_r2 = tuned_metrics['R2']\n",
    "improvement = ((tuned_gb_r2 - baseline_gb_r2) / baseline_gb_r2) * 100\n",
    "\n",
    "print(f\"\\n=== COMPARAISON BASELINE vs TUNED ===\")\n",
    "print(f\"Baseline R²: {baseline_gb_r2:.4f}\")\n",
    "print(f\"Tuned R²:    {tuned_gb_r2:.4f}\")\n",
    "print(f\"Amélioration: {improvement:.2f}%\")\n",
    "\n",
    "baseline_gb_rmse = baseline_metrics['Gradient Boosting']['RMSE']\n",
    "tuned_gb_rmse = tuned_metrics['RMSE']\n",
    "improvement_rmse = ((baseline_gb_rmse - tuned_gb_rmse) / baseline_gb_rmse) * 100\n",
    "\n",
    "print(f\"\\nBaseline RMSE: {baseline_gb_rmse:.4f}\")\n",
    "print(f\"Tuned RMSE:    {tuned_gb_rmse:.4f}\")\n",
    "print(f\"Amélioration:  {improvement_rmse:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ANALYSE D'ERREURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ANALYSE D'ERREURS ===\\n\")\n",
    "\n",
    "y_pred_tuned = tuned_metrics['y_pred']\n",
    "residuals = y_test.values - y_pred_tuned\n",
    "\n",
    "print(f\"Résidus (erreurs):\")\n",
    "print(f\"  Min: {residuals.min():.4f}\")\n",
    "print(f\"  Max: {residuals.max():.4f}\")\n",
    "print(f\"  Mean: {residuals.mean():.4f}\")\n",
    "print(f\"  Std: {residuals.std():.4f}\")\n",
    "\n",
    "# Percentiles d'erreur\n",
    "print(f\"\\nPercentiles d'erreur absolue:\")\n",
    "abs_errors = np.abs(residuals)\n",
    "for percentile in [25, 50, 75, 90, 95]:\n",
    "    value = np.percentile(abs_errors, percentile)\n",
    "    print(f\"  {percentile}e percentile: {value:.4f}\")\n",
    "\n",
    "# Pires prédictions\n",
    "print(f\"\\nTop 5 pires prédictions:\")\n",
    "worst_indices = np.argsort(abs_errors)[-5:][::-1]\n",
    "for rank, idx in enumerate(worst_indices, 1):\n",
    "    actual = y_test.values[idx]\n",
    "    pred = y_pred_tuned[idx]\n",
    "    error = residuals[idx]\n",
    "    print(f\"  {rank}. Actual: {actual:.2f}, Predicted: {pred:.2f}, Error: {error:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. VISUALISATION DES RÉSIDUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Résidus vs Prédictions\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_pred_tuned, residuals, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Prédictions')\n",
    "ax.set_ylabel('Résidus')\n",
    "ax.set_title('Résidus vs Prédictions')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des résidus\n",
    "ax = axes[0, 1]\n",
    "ax.hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.set_xlabel('Résidus')\n",
    "ax.set_ylabel('Fréquence')\n",
    "ax.set_title('Distribution des Résidus')\n",
    "ax.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "\n",
    "# Q-Q plot (normalité)\n",
    "ax = axes[1, 0]\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "ax.set_title('Q-Q Plot (Test de Normalité)')\n",
    "\n",
    "# Erreur absolue vs Valeurs réelles\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(y_test.values, abs_errors, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "ax.set_xlabel('Valeurs réelles')\n",
    "ax.set_ylabel('Erreur absolue')\n",
    "ax.set_title('Erreur Absolue vs Valeurs Réelles')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt
