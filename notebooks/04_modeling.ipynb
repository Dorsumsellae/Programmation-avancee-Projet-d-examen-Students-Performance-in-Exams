# ============================================================================
# NOTEBOOK 04 - MODELING.IPYNB - CODE COMPLET
# ============================================================================
# À copier dans Jupyter - Chaque section = une cellule
# ============================================================================

# CELLULE 1 : IMPORTS ET CONFIGURATION
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score, RandomizedSearchCV, KFold
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error
)
import joblib
import json
import time
import warnings
from scipy import stats
import os

warnings.filterwarnings('ignore')

# Configuration pour reproductibilité
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Style des graphiques
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Créer les dossiers s'ils n'existent pas
os.makedirs('models', exist_ok=True)
os.makedirs('reports', exist_ok=True)

print("✓ Imports réussis")
print(f"✓ Random state fixé à {RANDOM_STATE}")


# CELLULE 2 : CHARGEMENT DES DONNÉES PRÉTRAITÉES
# ============================================================================

# Les données doivent être prétraitées (voir notebook 03)
X_train = pd.read_csv('data/processed/X_train.csv')
X_test = pd.read_csv('data/processed/X_test.csv')
y_train = pd.read_csv('data/processed/y_train.csv').squeeze()
y_test = pd.read_csv('data/processed/y_test.csv').squeeze()

print("\n=== DONNÉES CHARGÉES ===")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Vérification
print(f"\nTarget statistics:")
print(f"  Min: {y_train.min():.2f}, Max: {y_train.max():.2f}")
print(f"  Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}")


# CELLULE 3 : ENTRAÎNEMENT DES MODÈLES BASELINE
# ============================================================================

print("\n=== ENTRAÎNEMENT DES MODÈLES BASELINE ===\n")

models = {}
baseline_results = {}
training_times = {}

# 1. Linear Regression
print("1. Linear Regression...")
start_time = time.time()
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
models['Linear Regression'] = model_lr
training_times['Linear Regression'] = time.time() - start_time
print(f"   ✓ Entraîné en {training_times['Linear Regression']:.4f}s")

# 2. Ridge Regression
print("2. Ridge Regression (α=1.0)...")
start_time = time.time()
model_ridge = Ridge(alpha=1.0, random_state=RANDOM_STATE)
model_ridge.fit(X_train, y_train)
models['Ridge (α=1.0)'] = model_ridge
training_times['Ridge (α=1.0)'] = time.time() - start_time
print(f"   ✓ Entraîné en {training_times['Ridge (α=1.0)']:.4f}s")

# 3. Lasso Regression
print("3. Lasso Regression (α=0.1)...")
start_time = time.time()
model_lasso = Lasso(alpha=0.1, random_state=RANDOM_STATE, max_iter=10000)
model_lasso.fit(X_train, y_train)
models['Lasso (α=0.1)'] = model_lasso
training_times['Lasso (α=0.1)'] = time.time() - start_time
print(f"   ✓ Entraîné en {training_times['Lasso (α=0.1)']:.4f}s")

# 4. Random Forest Regressor
print("4. Random Forest (100 arbres)...")
start_time = time.time()
model_rf = RandomForestRegressor(
    n_estimators=100,
    random_state=RANDOM_STATE,
    n_jobs=-1,
    max_depth=15
)
model_rf.fit(X_train, y_train)
models['Random Forest'] = model_rf
training_times['Random Forest'] = time.time() - start_time
print(f"   ✓ Entraîné en {training_times['Random Forest']:.4f}s")

# 5. Gradient Boosting Regressor
print("5. Gradient Boosting Regressor...")
start_time = time.time()
model_gb = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=RANDOM_STATE
)
model_gb.fit(X_train, y_train)
models['Gradient Boosting'] = model_gb
training_times['Gradient Boosting'] = time.time() - start_time
print(f"   ✓ Entraîné en {training_times['Gradient Boosting']:.4f}s")

print("\n✓ Tous les modèles baseline sont entraînés!")


# CELLULE 4 : VALIDATION CROISÉE (5-FOLD)
# ============================================================================

print("\n=== VALIDATION CROISÉE (5-FOLD) ===\n")

kfold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
cv_results = {}

for model_name, model in models.items():
    # Scores de CV
    cv_scores = cross_val_score(
        model, X_train, y_train,
        cv=kfold,
        scoring='r2',
        n_jobs=-1
    )
    
    cv_results[model_name] = {
        'mean_r2': cv_scores.mean(),
        'std_r2': cv_scores.std(),
        'scores': cv_scores.tolist()
    }
    
    print(f"{model_name}:")
    print(f"  R² CV: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})")
    print(f"  Scores: {[f'{s:.4f}' for s in cv_scores]}\n")


# CELLULE 5 : ÉVALUATION BASELINE SUR LE JEU DE TEST
# ============================================================================

print("\n=== ÉVALUATION BASELINE (TEST SET) ===\n")

def evaluate_model(model, X_test, y_test, model_name=""):
    """
    Évalue un modèle et retourne les métriques.
    """
    y_pred = model.predict(X_test)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    
    metrics = {
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2,
        'MAPE': mape,
        'y_pred': y_pred
    }
    
    return metrics

# Évaluer tous les modèles
baseline_metrics = {}
predictions_baseline = {}

for model_name, model in models.items():
    metrics = evaluate_model(model, X_test, y_test, model_name)
    baseline_metrics[model_name] = metrics
    predictions_baseline[model_name] = metrics.pop('y_pred')
    
    print(f"{model_name}:")
    print(f"  RMSE: {metrics['RMSE']:.4f}")
    print(f"  MAE:  {metrics['MAE']:.4f}")
    print(f"  R²:   {metrics['R2']:.4f}")
    print(f"  MAPE: {metrics['MAPE']:.4f}%\n")


# CELLULE 6 : COMPARAISON DES MODÈLES BASELINE
# ============================================================================

print("\n=== TABLEAU DE COMPARAISON DES MODÈLES ===\n")

comparison_df = pd.DataFrame({
    model_name: {
        'RMSE': baseline_metrics[model_name]['RMSE'],
        'MAE': baseline_metrics[model_name]['MAE'],
        'R²': baseline_metrics[model_name]['R2'],
        'MAPE': baseline_metrics[model_name]['MAPE'],
        'CV_R²': cv_results[model_name]['mean_r2'],
        'Training Time (s)': training_times[model_name]
    }
    for model_name in models.keys()
}).T

# Arrondir et trier par R²
comparison_df = comparison_df.round(4)
comparison_df_sorted = comparison_df.sort_values('R²', ascending=False)

print(comparison_df_sorted)
print(f"\n✓ Meilleur modèle baseline: {comparison_df_sorted.index[0]}")

# Sauvegarder le tableau
comparison_df_sorted.to_csv('reports/baseline_comparison.csv')
print("✓ Tableau sauvegardé dans reports/baseline_comparison.csv")


# CELLULE 7 : VISUALISATION - COMPARAISON DES MODÈLES
# ============================================================================

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# RMSE comparison
ax = axes[0]
models_names = list(baseline_metrics.keys())
rmse_values = [baseline_metrics[m]['RMSE'] for m in models_names]
colors = ['#FF6B6B' if v == max(rmse_values) else '#4ECDC4' if v == min(rmse_values) else '#95E1D3' 
          for v in rmse_values]
ax.barh(models_names, rmse_values, color=colors)
ax.set_xlabel('RMSE (plus bas = mieux)')
ax.set_title('Comparaison RMSE - Baseline')
ax.invert_yaxis()

# R² comparison
ax = axes[1]
r2_values = [baseline_metrics[m]['R2'] for m in models_names]
colors = ['#4ECDC4' if v == max(r2_values) else '#FF6B6B' if v == min(r2_values) else '#95E1D3' 
          for v in r2_values]
ax.barh(models_names, r2_values, color=colors)
ax.set_xlabel('R² (plus haut = mieux)')
ax.set_title('Comparaison R² - Baseline')
ax.set_xlim(0.6, 0.8)
ax.invert_yaxis()

# MAE comparison
ax = axes[2]
mae_values = [baseline_metrics[m]['MAE'] for m in models_names]
colors = ['#FF6B6B' if v == max(mae_values) else '#4ECDC4' if v == min(mae_values) else '#95E1D3' 
          for v in mae_values]
ax.barh(models_names, mae_values, color=colors)
ax.set_xlabel('MAE (plus bas = mieux)')
ax.set_title('Comparaison MAE - Baseline')
ax.invert_yaxis()

plt.tight_layout()
plt.savefig('reports/baseline_models_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Graphique sauvegardé dans reports/baseline_models_comparison.png")


# CELLULE 8 : TUNING D'HYPERPARAMÈTRES (GRADIENT BOOSTING)
# ============================================================================

print("\n=== HYPERPARAMETER TUNING - GRADIENT BOOSTING ===\n")

print("Modèle choisi pour le tuning: Gradient Boosting")
print("Raison: Meilleur R² et RMSE parmi les modèles baseline\n")

# Espace de recherche
param_dist = {
    'n_estimators': [100, 150, 200, 250, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
    'max_depth': [3, 4, 5, 6, 7],
    'min_samples_split': [2, 5, 10, 15],
    'min_samples_leaf': [1, 2, 4],
    'subsample': [0.8, 0.85, 0.9, 0.95, 1.0],
    'max_features': ['sqrt', 'log2']
}

print("Espace de recherche:")
for param, values in param_dist.items():
    print(f"  {param}: {values}")

print("\nLancement de RandomizedSearchCV (50 itérations)...")
print("Métrique: Negative RMSE (neg_root_mean_squared_error)")
print("CV: 5-fold\n")

start_time = time.time()
random_search = RandomizedSearchCV(
    GradientBoostingRegressor(random_state=RANDOM_STATE),
    param_dist,
    n_iter=50,
    cv=5,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    random_state=RANDOM_STATE,
    verbose=1
)

random_search.fit(X_train, y_train)
tuning_time = time.time() - start_time

print(f"\n✓ Tuning complété en {tuning_time/60:.2f} minutes")

# Résultats du tuning
print(f"\nMeilleur RMSE (CV): {-random_search.best_score_:.4f}")
print(f"Meilleur R² (CV): {random_search.best_estimator_.score(X_train, y_train):.4f}")

print("\nMeilleurs hyperparamètres:")
for param, value in random_search.best_params_.items():
    print(f"  {param}: {value}")


# CELLULE 9 : ÉVALUATION DU MODÈLE TUNÉ
# ============================================================================

print("\n=== ÉVALUATION DU MODÈLE TUNÉ ===\n")

best_model = random_search.best_estimator_
tuned_metrics = evaluate_model(best_model, X_test, y_test, "Gradient Boosting (tuned)")

print(f"Gradient Boosting (tuned):")
print(f"  RMSE: {tuned_metrics['RMSE']:.4f}")
print(f"  MAE:  {tuned_metrics['MAE']:.4f}")
print(f"  R²:   {tuned_metrics['R2']:.4f}")
print(f"  MAPE: {tuned_metrics['MAPE']:.4f}%")

# Comparaison baseline vs tuned
baseline_gb_r2 = baseline_metrics['Gradient Boosting']['R2']
tuned_gb_r2 = tuned_metrics['R2']
improvement = ((tuned_gb_r2 - baseline_gb_r2) / baseline_gb_r2) * 100

print(f"\n=== COMPARAISON BASELINE vs TUNED ===")
print(f"Baseline R²: {baseline_gb_r2:.4f}")
print(f"Tuned R²:    {tuned_gb_r2:.4f}")
print(f"Amélioration: {improvement:.2f}%")

baseline_gb_rmse = baseline_metrics['Gradient Boosting']['RMSE']
tuned_gb_rmse = tuned_metrics['RMSE']
improvement_rmse = ((baseline_gb_rmse - tuned_gb_rmse) / baseline_gb_rmse) * 100

print(f"\nBaseline RMSE: {baseline_gb_rmse:.4f}")
print(f"Tuned RMSE:    {tuned_gb_rmse:.4f}")
print(f"Amélioration:  {improvement_rmse:.2f}%")


# CELLULE 10 : ANALYSE D'ERREURS
# ============================================================================

print("\n=== ANALYSE D'ERREURS ===\n")

y_pred_tuned = tuned_metrics['y_pred']
residuals = y_test.values - y_pred_tuned

print(f"Résidus (erreurs):")
print(f"  Min: {residuals.min():.4f}")
print(f"  Max: {residuals.max():.4f}")
print(f"  Mean: {residuals.mean():.4f}")
print(f"  Std: {residuals.std():.4f}")

# Percentiles d'erreur
print(f"\nPercentiles d'erreur absolue:")
abs_errors = np.abs(residuals)
for percentile in [25, 50, 75, 90, 95]:
    value = np.percentile(abs_errors, percentile)
    print(f"  {percentile}e percentile: {value:.4f}")

# Pires prédictions
print(f"\nTop 5 pires prédictions:")
worst_indices = np.argsort(abs_errors)[-5:][::-1]
for rank, idx in enumerate(worst_indices, 1):
    actual = y_test.values[idx]
    pred = y_pred_tuned[idx]
    error = residuals[idx]
    print(f"  {rank}. Actual: {actual:.2f}, Predicted: {pred:.2f}, Error: {error:+.2f}")


# CELLULE 11 : VISUALISATION DES RÉSIDUS
# ============================================================================

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Résidus vs Prédictions
ax = axes[0, 0]
ax.scatter(y_pred_tuned, residuals, alpha=0.6, edgecolors='k', linewidth=0.5)
ax.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax.set_xlabel('Prédictions')
ax.set_ylabel('Résidus')
ax.set_title('Résidus vs Prédictions')
ax.grid(True, alpha=0.3)

# Distribution des résidus
ax = axes[0, 1]
ax.hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='steelblue')
ax.set_xlabel('Résidus')
ax.set_ylabel('Fréquence')
ax.set_title('Distribution des Résidus')
ax.axvline(x=0, color='r', linestyle='--', linewidth=2)

# Q-Q plot (normalité)
ax = axes[1, 0]
stats.probplot(residuals, dist="norm", plot=ax)
ax.set_title('Q-Q Plot (Test de Normalité)')

# Erreur absolue vs Valeurs réelles
ax = axes[1, 1]
ax.scatter(y_test.values, abs_errors, alpha=0.6, edgecolors='k', linewidth=0.5)
ax.set_xlabel('Valeurs réelles')
ax.set_ylabel('Erreur absolue')
ax.set_title('Erreur Absolue vs Valeurs Réelles')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('reports/error_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Graphique d'analyse d'erreurs sauvegardé")


# CELLULE 12 : FEATURE IMPORTANCE
# ============================================================================

print("\n=== FEATURE IMPORTANCE ===\n")

# Importance des features (Gradient Boosting)
feature_importance = best_model.feature_importances_
feature_names = X_train.columns

# Créer DataFrame
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance,
    'importance_percent': feature_importance / feature_importance.sum() * 100
}).sort_values('importance', ascending=False)

print("Top 10 features:")
print(importance_df.head(10).to_string(index=False))

# Sauvegarder
importance_df.to_csv('reports/feature_importance.csv', index=False)

# Visualisation
fig, ax = plt.subplots(figsize=(10, 8))
top_n = 10
top_features = importance_df.head(top_n)

colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))
ax.barh(range(len(top_features)), top_features['importance_percent'], color=colors)
ax.set_yticks(range(len(top_features)))
ax.set_yticklabels(top_features['feature'])
ax.set_xlabel('Importance (%)')
ax.set_title(f'Top {top_n} Features - Gradient Boosting')
ax.invert_yaxis()

# Ajouter les valeurs
for i, (idx, row) in enumerate(top_features.iterrows()):
    ax.text(row['importance_percent'] + 0.5, i, f"{row['importance_percent']:.1f}%", 
            va='center', fontsize=10)

plt.tight_layout()
plt.savefig('reports/feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Graphique feature importance sauvegardé")


# CELLULE 13 : PRÉDICTIONS vs RÉALITÉ
# ============================================================================

fig, ax = plt.subplots(figsize=(10, 8))

# Scatter plot
ax.scatter(y_test.values, y_pred_tuned, alpha=0.6, edgecolors='k', linewidth=0.5, s=50)

# Ligne de référence (y=x)
min_val = min(y_test.min(), y_pred_tuned.min())
max_val = max(y_test.max(), y_pred_tuned.max())
ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Prédiction parfaite')

ax.set_xlabel('Valeurs réelles', fontsize=12)
ax.set_ylabel('Prédictions', fontsize=12)
ax.set_title(f'Prédictions vs Réalité (R² = {tuned_metrics["R2"]:.4f})', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('reports/predictions_vs_reality.png', dpi=300, bbox_inches='tight')
plt.show()
print("✓ Graphique prédictions vs réalité sauvegardé")


# CELLULE 14 : SAUVEGARDE DES MODÈLES ET MÉTADONNÉES
# ============================================================================

print("\n=== SAUVEGARDE DES MODÈLES ===\n")

# Créer répertoire models s'il n'existe pas
os.makedirs('models', exist_ok=True)

# Sauvegarder le meilleur modèle
joblib.dump(best_model, 'models/best_model.pkl')
print("✓ best_model.pkl sauvegardé")

# Sauvegarder tous les modèles baseline
joblib.dump(models, 'models/baseline_models.pkl')
print("✓ baseline_models.pkl sauvegardé")

# Sauvegarder résultats de comparaison
comparison_dict = {}
for model_name in models.keys():
    comparison_dict[model_name] = {
        'rmse': float(baseline_metrics[model_name]['RMSE']),
        'mae': float(baseline_metrics[model_name]['MAE']),
        'r2': float(baseline_metrics[model_name]['R2']),
        'mape': float(baseline_metrics[model_name]['MAPE']),
        'training_time': float(training_times[model_name])
    }

with open('models/model_comparison.json', 'w') as f:
    json.dump(comparison_dict, f, indent=2)
print("✓ model_comparison.json sauvegardé")

# Sauvegarder résultats du tuning
tuning_dict = {
    'method': 'RandomizedSearchCV',
    'cv_folds': 5,
    'n_iterations': 50,
    'metric': 'neg_root_mean_squared_error',
    'baseline_r2': float(baseline_gb_r2),
    'tuned_r2': float(tuned_gb_r2),
    'improvement_percent': float(improvement),
    'baseline_rmse': float(baseline_gb_rmse),
    'tuned_rmse': float(tuned_gb_rmse),
    'improvement_rmse_percent': float(improvement_rmse),
    'best_params': {k: (str(v) if not isinstance(v, (int, float)) else v) for k, v in random_search.best_params_.items()},
    'best_cv_rmse': float(-random_search.best_score_),
    'final_test_rmse': float(tuned_metrics['RMSE']),
    'final_test_r2': float(tuned_metrics['R2']),
    'final_test_mae': float(tuned_metrics['MAE']),
    'tuning_time_minutes': float(tuning_time / 60)
}

with open('models/hyperparameter_tuning.json', 'w') as f:
    json.dump(tuning_dict, f, indent=2)
print("✓ hyperparameter_tuning.json sauvegardé")

# Sauvegarder feature importance
importance_dict = {
    'method': 'Gradient Boosting Feature Importance',
    'top_features': []
}

for rank, (idx, row) in enumerate(importance_df.head(10).iterrows(), 1):
    importance_dict['top_features'].append({
        'rank': rank,
        'feature': row['feature'],
        'importance': float(row['importance']),
        'importance_percent': float(row['importance_percent'])
    })

with open('models/feature_importance.json', 'w') as f:
    json.dump(importance_dict, f, indent=2)
print("✓ feature_importance.json sauvegardé")


# CELLULE 15 : RÉSUMÉ FINAL
# ============================================================================

print("\n" + "="*70)
print("RÉSUMÉ FINAL - MODELING")
print("="*70)

print("\n MODÈLES ENTRAÎNÉS:")
for model_name in models.keys():
    print(f"  ✓ {model_name}")

print("\n MEILLEUR MODÈLE: Gradient Boosting (tuned)")
print(f"   R²:   {tuned_metrics['R2']:.4f} ({improvement:.2f}% mieux que baseline)")
print(f"   RMSE: {tuned_metrics['RMSE']:.4f} ({improvement_rmse:.2f}% mieux que baseline)")
print(f"   MAE:  {tuned_metrics['MAE']:.4f}")

print("\n  HYPERPARAMÈTRES OPTIMAUX:")
for param, value in random_search.best_params_.items():
    print(f"   {param}: {value}")

print("\n TOP 5 FEATURES:")
for rank, (idx, row) in enumerate(importance_df.head(5).iterrows(), 1):
    print(f"   {rank}. {row['feature']}: {row['importance_percent']:.2f}%")

print("\n FICHIERS SAUVEGARDÉS:")
print("   ✓ models/best_model.pkl")
print("   ✓ models/baseline_models.pkl")
print("   ✓ models/model_comparison.json")
print("   ✓ models/hyperparameter_tuning.json")
print("   ✓ models/feature_importance.json")
print("   ✓ reports/baseline_comparison.csv")
print("   ✓ reports/baseline_models_comparison.png")
print("   ✓ reports/error_analysis.png")
print("   ✓ reports/feature_importance.png")
print("   ✓ reports/feature_importance.csv")
print("   ✓ reports/predictions_vs_reality.png")

print("\n" + "="*70)
print(" MODELING TERMINÉ AVEC SUCCÈS!")
print("="*70)
