# 04_modeling.ipynb - Mod√©lisation & R√©gression Compl√®te
# =====================================================
# Ce notebook entra√Æne plusieurs mod√®les de r√©gression,
# effectue un tuning d'hyperparam√®tres, et s√©lectionne le meilleur mod√®le.

"""
STRUCTURE DU NOTEBOOK:
1. Imports et configuration
2. Chargement des donn√©es pr√©trait√©es
3. Entra√Ænement des mod√®les baseline
4. Validation crois√©e
5. Tuning des hyperparam√®tres
6. √âvaluation et comparaison
7. Analyse d'erreurs
8. Feature importance
9. Sauvegarde des mod√®les
"""

# ============================================================================
# 1. IMPORTS ET CONFIGURATION
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score, RandomizedSearchCV, KFold
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error
)
import joblib
import json
import time
import warnings
warnings.filterwarnings('ignore')

# Configuration pour reproductibilit√©
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Style des graphiques
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("‚úì Imports r√©ussis")
print(f"‚úì Random state fix√© √† {RANDOM_STATE}")

# ============================================================================
# 2. CHARGEMENT DES DONN√âES PR√âTRAIT√âES
# ============================================================================

# Les donn√©es doivent √™tre pr√©trait√©es (voir notebook 03)
X_train = pd.read_csv('data/processed/X_train.csv')
X_test = pd.read_csv('data/processed/X_test.csv')
y_train = pd.read_csv('data/processed/y_train.csv').squeeze()
y_test = pd.read_csv('data/processed/y_test.csv').squeeze()

print("\n=== DONN√âES CHARG√âES ===")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# V√©rification
print(f"\nTarget statistics:")
print(f"  Min: {y_train.min():.2f}, Max: {y_train.max():.2f}")
print(f"  Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}")

# ============================================================================
# 3. ENTRA√éNEMENT DES MOD√àLES BASELINE
# ============================================================================

print("\n=== ENTRA√éNEMENT DES MOD√àLES BASELINE ===\n")

models = {}
baseline_results = {}
training_times = {}

# 3.1 Linear Regression
print("1. Linear Regression...")
start_time = time.time()
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
models['Linear Regression'] = model_lr
training_times['Linear Regression'] = time.time() - start_time
print(f"   ‚úì Entra√Æn√© en {training_times['Linear Regression']:.4f}s")

# 3.2 Ridge Regression
print("2. Ridge Regression (Œ±=1.0)...")
start_time = time.time()
model_ridge = Ridge(alpha=1.0, random_state=RANDOM_STATE)
model_ridge.fit(X_train, y_train)
models['Ridge (Œ±=1.0)'] = model_ridge
training_times['Ridge (Œ±=1.0)'] = time.time() - start_time
print(f"   ‚úì Entra√Æn√© en {training_times['Ridge (Œ±=1.0)']:.4f}s")

# 3.3 Lasso Regression
print("3. Lasso Regression (Œ±=0.1)...")
start_time = time.time()
model_lasso = Lasso(alpha=0.1, random_state=RANDOM_STATE, max_iter=10000)
model_lasso.fit(X_train, y_train)
models['Lasso (Œ±=0.1)'] = model_lasso
training_times['Lasso (Œ±=0.1)'] = time.time() - start_time
print(f"   ‚úì Entra√Æn√© en {training_times['Lasso (Œ±=0.1)']:.4f}s")

# 3.4 Random Forest Regressor
print("4. Random Forest (100 arbres)...")
start_time = time.time()
model_rf = RandomForestRegressor(
    n_estimators=100,
    random_state=RANDOM_STATE,
    n_jobs=-1,
    max_depth=15
)
model_rf.fit(X_train, y_train)
models['Random Forest'] = model_rf
training_times['Random Forest'] = time.time() - start_time
print(f"   ‚úì Entra√Æn√© en {training_times['Random Forest']:.4f}s")

# 3.5 Gradient Boosting Regressor
print("5. Gradient Boosting Regressor...")
start_time = time.time()
model_gb = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=RANDOM_STATE
)
model_gb.fit(X_train, y_train)
models['Gradient Boosting'] = model_gb
training_times['Gradient Boosting'] = time.time() - start_time
print(f"   ‚úì Entra√Æn√© en {training_times['Gradient Boosting']:.4f}s")

print("\n‚úì Tous les mod√®les baseline sont entra√Æn√©s!")

# ============================================================================
# 4. VALIDATION CROIS√âE (5-FOLD)
# ============================================================================

print("\n=== VALIDATION CROIS√âE (5-FOLD) ===\n")

kfold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
cv_results = {}

for model_name, model in models.items():
    # Scores de CV
    cv_scores = cross_val_score(
        model, X_train, y_train,
        cv=kfold,
        scoring='r2',
        n_jobs=-1
    )
    
    cv_results[model_name] = {
        'mean_r2': cv_scores.mean(),
        'std_r2': cv_scores.std(),
        'scores': cv_scores.tolist()
    }
    
    print(f"{model_name}:")
    print(f"  R¬≤ CV: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})")
    print(f"  Scores: {[f'{s:.4f}' for s in cv_scores]}")

# ============================================================================
# 5. √âVALUATION BASELINE SUR LE JEUX DE TEST
# ============================================================================

print("\n=== √âVALUATION BASELINE (TEST SET) ===\n")

def evaluate_model(model, X_test, y_test, model_name=""):
    """
    √âvalue un mod√®le et retourne les m√©triques.
    """
    y_pred = model.predict(X_test)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    
    metrics = {
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2,
        'MAPE': mape,
        'y_pred': y_pred
    }
    
    return metrics

# √âvaluer tous les mod√®les
baseline_metrics = {}
predictions_baseline = {}

for model_name, model in models.items():
    metrics = evaluate_model(model, X_test, y_test, model_name)
    baseline_metrics[model_name] = metrics
    predictions_baseline[model_name] = metrics.pop('y_pred')
    
    print(f"{model_name}:")
    print(f"  RMSE: {metrics['RMSE']:.4f}")
    print(f"  MAE:  {metrics['MAE']:.4f}")
    print(f"  R¬≤:   {metrics['R2']:.4f}")
    print(f"  MAPE: {metrics['MAPE']:.4f}%")

# ============================================================================
# 6. COMPARAISON DES MOD√àLES BASELINE
# ============================================================================

print("\n=== TABLEAU DE COMPARAISON DES MOD√àLES ===\n")

comparison_df = pd.DataFrame({
    model_name: {
        'RMSE': baseline_metrics[model_name]['RMSE'],
        'MAE': baseline_metrics[model_name]['MAE'],
        'R¬≤': baseline_metrics[model_name]['R2'],
        'MAPE': baseline_metrics[model_name]['MAPE'],
        'CV_R¬≤': cv_results[model_name]['mean_r2'],
        'Training Time (s)': training_times[model_name]
    }
    for model_name in models.keys()
}).T

# Arrondir et trier par R¬≤
comparison_df = comparison_df.round(4)
comparison_df_sorted = comparison_df.sort_values('R¬≤', ascending=False)

print(comparison_df_sorted)
print(f"\n‚úì Meilleur mod√®le baseline: {comparison_df_sorted.index[0]}")

# Sauvegarder le tableau
comparison_df_sorted.to_csv('reports/baseline_comparison.csv')
print("‚úì Tableau sauvegard√© dans reports/baseline_comparison.csv")

# ============================================================================
# 7. VISUALISATION - COMPARAISON DES MOD√àLES
# ============================================================================

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# RMSE comparison
ax = axes[0]
models_names = list(baseline_metrics.keys())
rmse_values = [baseline_metrics[m]['RMSE'] for m in models_names]
colors = ['#FF6B6B' if v == max(rmse_values) else '#4ECDC4' if v == min(rmse_values) else '#95E1D3' 
          for v in rmse_values]
ax.barh(models_names, rmse_values, color=colors)
ax.set_xlabel('RMSE (plus bas = mieux)')
ax.set_title('Comparaison RMSE - Baseline')
ax.invert_yaxis()

# R¬≤ comparison
ax = axes[1]
r2_values = [baseline_metrics[m]['R2'] for m in models_names]
colors = ['#4ECDC4' if v == max(r2_values) else '#FF6B6B' if v == min(r2_values) else '#95E1D3' 
          for v in r2_values]
ax.barh(models_names, r2_values, color=colors)
ax.set_xlabel('R¬≤ (plus haut = mieux)')
ax.set_title('Comparaison R¬≤ - Baseline')
ax.set_xlim(0.6, 0.8)
ax.invert_yaxis()

# MAE comparison
ax = axes[2]
mae_values = [baseline_metrics[m]['MAE'] for m in models_names]
colors = ['#FF6B6B' if v == max(mae_values) else '#4ECDC4' if v == min(mae_values) else '#95E1D3' 
          for v in mae_values]
ax.barh(models_names, mae_values, color=colors)
ax.set_xlabel('MAE (plus bas = mieux)')
ax.set_title('Comparaison MAE - Baseline')
ax.invert_yaxis()

plt.tight_layout()
plt.savefig('reports/baseline_models_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
print("‚úì Graphique sauvegard√© dans reports/baseline_models_comparison.png")

# ============================================================================
# 8. TUNING D'HYPERPARAM√àTRES (GRADIENT BOOSTING)
# ============================================================================

print("\n=== HYPERPARAMETER TUNING - GRADIENT BOOSTING ===\n")

# Param√®tres de base du meilleur mod√®le
print("Mod√®le choisi pour le tuning: Gradient Boosting")
print("Raison: Meilleur R¬≤ et RMSE parmi les mod√®les baseline\n")

# Espace de recherche
param_dist = {
    'n_estimators': [100, 150, 200, 250, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
    'max_depth': [3, 4, 5, 6, 7],
    'min_samples_split': [2, 5, 10, 15],
    'min_samples_leaf': [1, 2, 4],
    'subsample': [0.8, 0.85, 0.9, 0.95, 1.0],
    'max_features': ['sqrt', 'log2']
}

print("Espace de recherche:")
for param, values in param_dist.items():
    print(f"  {param}: {values}")

# RandomizedSearchCV
print("\nLancement de RandomizedSearchCV (50 it√©rations)...")
print("M√©trique: Negative RMSE (neg_root_mean_squared_error)")
print("CV: 5-fold\n")

start_time = time.time()
random_search = RandomizedSearchCV(
    GradientBoostingRegressor(random_state=RANDOM_STATE),
    param_dist,
    n_iter=50,
    cv=5,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    random_state=RANDOM_STATE,
    verbose=1
)

random_search.fit(X_train, y_train)
tuning_time = time.time() - start_time

print(f"\n‚úì Tuning compl√©t√© en {tuning_time/60:.2f} minutes")

# R√©sultats du tuning
print(f"\nMeilleur RMSE (CV): {-random_search.best_score_:.4f}")
print(f"Meilleur R¬≤ (CV): {random_search.best_estimator_.score(X_train, y_train):.4f}")

print("\nMeilleurs hyperparam√®tres:")
for param, value in random_search.best_params_.items():
    print(f"  {param}: {value}")

# ============================================================================
# 9. √âVALUATION DU MOD√àLE TUN√â
# ============================================================================

print("\n=== √âVALUATION DU MOD√àLE TUN√â ===\n")

best_model = random_search.best_estimator_
tuned_metrics = evaluate_model(best_model, X_test, y_test, "Gradient Boosting (tuned)")

print(f"Gradient Boosting (tuned):")
print(f"  RMSE: {tuned_metrics['RMSE']:.4f}")
print(f"  MAE:  {tuned_metrics['MAE']:.4f}")
print(f"  R¬≤:   {tuned_metrics['R2']:.4f}")
print(f"  MAPE: {tuned_metrics['MAPE']:.4f}%")

# Comparaison baseline vs tuned
baseline_gb_r2 = baseline_metrics['Gradient Boosting']['R2']
tuned_gb_r2 = tuned_metrics['R2']
improvement = ((tuned_gb_r2 - baseline_gb_r2) / baseline_gb_r2) * 100

print(f"\n=== COMPARAISON BASELINE vs TUNED ===")
print(f"Baseline R¬≤: {baseline_gb_r2:.4f}")
print(f"Tuned R¬≤:    {tuned_gb_r2:.4f}")
print(f"Am√©lioration: {improvement:.2f}%")

baseline_gb_rmse = baseline_metrics['Gradient Boosting']['RMSE']
tuned_gb_rmse = tuned_metrics['RMSE']
improvement_rmse = ((baseline_gb_rmse - tuned_gb_rmse) / baseline_gb_rmse) * 100

print(f"\nBaseline RMSE: {baseline_gb_rmse:.4f}")
print(f"Tuned RMSE:    {tuned_gb_rmse:.4f}")
print(f"Am√©lioration:  {improvement_rmse:.2f}%")

# ============================================================================
# 10. ANALYSE D'ERREURS
# ============================================================================

print("\n=== ANALYSE D'ERREURS ===\n")

y_pred_tuned = tuned_metrics['y_pred']
residuals = y_test.values - y_pred_tuned

print(f"R√©sidus (erreurs):")
print(f"  Min: {residuals.min():.4f}")
print(f"  Max: {residuals.max():.4f}")
print(f"  Mean: {residuals.mean():.4f}")
print(f"  Std: {residuals.std():.4f}")

# Percentiles d'erreur
print(f"\nPercentiles d'erreur absolue:")
abs_errors = np.abs(residuals)
for percentile in [25, 50, 75, 90, 95]:
    value = np.percentile(abs_errors, percentile)
    print(f"  {percentile}e percentile: {value:.4f}")

# Pires pr√©dictions
print(f"\nTop 5 pires pr√©dictions:")
worst_indices = np.argsort(abs_errors)[-5:][::-1]
for rank, idx in enumerate(worst_indices, 1):
    actual = y_test.values[idx]
    pred = y_pred_tuned[idx]
    error = residuals[idx]
    print(f"  {rank}. Actual: {actual:.2f}, Predicted: {pred:.2f}, Error: {error:+.2f}")

# Visualisation r√©sidus
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# R√©sidus vs Pr√©dictions
ax = axes[0, 0]
ax.scatter(y_pred_tuned, residuals, alpha=0.6, edgecolors='k', linewidth=0.5)
ax.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax.set_xlabel('Pr√©dictions')
ax.set_ylabel('R√©sidus')
ax.set_title('R√©sidus vs Pr√©dictions')
ax.grid(True, alpha=0.3)

# Distribution des r√©sidus
ax = axes[0, 1]
ax.hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='steelblue')
ax.set_xlabel('R√©sidus')
ax.set_ylabel('Fr√©quence')
ax.set_title('Distribution des R√©sidus')
ax.axvline(x=0, color='r', linestyle='--', linewidth=2)

# Q-Q plot (normalit√©)
from scipy import stats
ax = axes[1, 0]
stats.probplot(residuals, dist="norm", plot=ax)
ax.set_title('Q-Q Plot (Test de Normalit√©)')

# Erreur absolue vs Valeurs r√©elles
ax = axes[1, 1]
ax.scatter(y_test.values, abs_errors, alpha=0.6, edgecolors='k', linewidth=0.5)
ax.set_xlabel('Valeurs r√©elles')
ax.set_ylabel('Erreur absolue')
ax.set_title('Erreur Absolue vs Valeurs R√©elles')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('reports/error_analysis.png', dpi=300, bbox_inches='tight')
plt.show()
print("‚úì Graphique d'analyse d'erreurs sauvegard√©")

# ============================================================================
# 11. FEATURE IMPORTANCE
# ============================================================================

print("\n=== FEATURE IMPORTANCE ===\n")

# Importance des features (Gradient Boosting)
feature_importance = best_model.feature_importances_
feature_names = X_train.columns

# Cr√©er DataFrame
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance,
    'importance_percent': feature_importance / feature_importance.sum() * 100
}).sort_values('importance', ascending=False)

print("Top 10 features:")
print(importance_df.head(10).to_string(index=False))

# Sauvegarder
importance_df.to_csv('reports/feature_importance.csv', index=False)

# Visualisation
fig, ax = plt.subplots(figsize=(10, 8))
top_n = 10
top_features = importance_df.head(top_n)

colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))
ax.barh(range(len(top_features)), top_features['importance_percent'], color=colors)
ax.set_yticks(range(len(top_features)))
ax.set_yticklabels(top_features['feature'])
ax.set_xlabel('Importance (%)')
ax.set_title(f'Top {top_n} Features - Gradient Boosting')
ax.invert_yaxis()

# Ajouter les valeurs
for i, (idx, row) in enumerate(top_features.iterrows()):
    ax.text(row['importance_percent'] + 0.5, i, f"{row['importance_percent']:.1f}%", 
            va='center', fontsize=10)

plt.tight_layout()
plt.savefig('reports/feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()
print("‚úì Graphique feature importance sauvegard√©")

# ============================================================================
# 12. PR√âDICTIONS vs R√âALIT√â
# ============================================================================

print("\n=== PR√âDICTIONS vs R√âALIT√â ===\n")

fig, ax = plt.subplots(figsize=(10, 8))

# Scatter plot
ax.scatter(y_test.values, y_pred_tuned, alpha=0.6, edgecolors='k', linewidth=0.5, s=50)

# Ligne de r√©f√©rence (y=x)
min_val = min(y_test.min(), y_pred_tuned.min())
max_val = max(y_test.max(), y_pred_tuned.max())
ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Pr√©diction parfaite')

ax.set_xlabel('Valeurs r√©elles', fontsize=12)
ax.set_ylabel('Pr√©dictions', fontsize=12)
ax.set_title(f'Pr√©dictions vs R√©alit√© (R¬≤ = {tuned_metrics["R2"]:.4f})', fontsize=14)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('reports/predictions_vs_reality.png', dpi=300, bbox_inches='tight')
plt.show()
print("‚úì Graphique pr√©dictions vs r√©alit√© sauvegard√©")

# ============================================================================
# 13. SAUVEGARDE DES MOD√àLES ET M√âTADONN√âES
# ============================================================================

print("\n=== SAUVEGARDE DES MOD√àLES ===\n")

# Cr√©er r√©pertoire models s'il n'existe pas
import os
os.makedirs('models', exist_ok=True)

# Sauvegarder le meilleur mod√®le
joblib.dump(best_model, 'models/best_model.pkl')
print("‚úì best_model.pkl sauvegard√©")

# Sauvegarder tous les mod√®les baseline
joblib.dump(models, 'models/baseline_models.pkl')
print("‚úì baseline_models.pkl sauvegard√©")

# Sauvegarder scaler et encoder (si disponibles)
# √Ä adapter selon votre preprocessing
scaler = None
encoder = None

try:
    scaler = joblib.load('models/scaler.pkl')
    print("‚úì scaler.pkl trouv√©")
except:
    print("‚ö† scaler.pkl non trouv√© (adapter le chemin si n√©cessaire)")

try:
    encoder = joblib.load('models/encoder.pkl')
    print("‚úì encoder.pkl trouv√©")
except:
    print("‚ö† encoder.pkl non trouv√© (adapter le chemin si n√©cessaire)")

# Sauvegarder r√©sultats de comparaison
comparison_dict = {}
for model_name in models.keys():
    comparison_dict[model_name] = {
        'rmse': float(baseline_metrics[model_name]['RMSE']),
        'mae': float(baseline_metrics[model_name]['MAE']),
        'r2': float(baseline_metrics[model_name]['R2']),
        'mape': float(baseline_metrics[model_name]['MAPE']),
        'training_time': float(training_times[model_name])
    }

with open('models/model_comparison.json', 'w') as f:
    json.dump(comparison_dict, f, indent=2)
print("‚úì model_comparison.json sauvegard√©")

# Sauvegarder r√©sultats du tuning
tuning_dict = {
    'method': 'RandomizedSearchCV',
    'cv_folds': 5,
    'n_iterations': 50,
    'metric': 'neg_root_mean_squared_error',
    'baseline_r2': float(baseline_gb_r2),
    'tuned_r2': float(tuned_gb_r2),
    'improvement_percent': float(improvement),
    'baseline_rmse': float(baseline_gb_rmse),
    'tuned_rmse': float(tuned_gb_rmse),
    'improvement_rmse_percent': float(improvement_rmse),
    'best_params': random_search.best_params_,
    'best_cv_rmse': float(-random_search.best_score_),
    'final_test_rmse': float(tuned_metrics['RMSE']),
    'final_test_r2': float(tuned_metrics['R2']),
    'final_test_mae': float(tuned_metrics['MAE']),
    'tuning_time_minutes': float(tuning_time / 60)
}

with open('models/hyperparameter_tuning.json', 'w') as f:
    json.dump(tuning_dict, f, indent=2)
print("‚úì hyperparameter_tuning.json sauvegard√©")

# Sauvegarder feature importance
importance_dict = {
    'method': 'Gradient Boosting Feature Importance',
    'top_features': []
}

for rank, (idx, row) in enumerate(importance_df.head(10).iterrows(), 1):
    importance_dict['top_features'].append({
        'rank': rank,
        'feature': row['feature'],
        'importance': float(row['importance']),
        'importance_percent': float(row['importance_percent'])
    })

with open('models/feature_importance.json', 'w') as f:
    json.dump(importance_dict, f, indent=2)
print("‚úì feature_importance.json sauvegard√©")

# ============================================================================
# 14. R√âSUM√â FINAL
# ============================================================================

print("\n" + "="*70)
print("R√âSUM√â FINAL - MODELING")
print("="*70)

print("\nüìä MOD√àLES ENTRA√éN√âS:")
for model_name in models.keys():
    print(f"  ‚úì {model_name}")

print("\nüèÜ MEILLEUR MOD√àLE: Gradient Boosting (tuned)")
print(f"   R¬≤:   {tuned_metrics['R2']:.4f} ({improvement:.2f}% mieux que baseline)")
print(f"   RMSE: {tuned_metrics['RMSE']:.4f} ({improvement_rmse:.2f}% mieux que baseline)")
print(f"   MAE:  {tuned_metrics['MAE']:.4f}")

print("\n‚öôÔ∏è  HYPERPARAM√àTRES OPTIMAUX:")
for param, value in random_search.best_params_.items():
    print(f"   {param}: {value}")

print("\nüìà TOP 5 FEATURES:")
for rank, (idx, row) in enumerate(importance_df.head(5).iterrows(), 1):
    print(f"   {rank}. {row['feature']}: {row['importance_percent']:.2f}%")

print("\nüíæ FICHIERS SAUVEGARD√âS:")
print("   ‚úì models/best_model.pkl")
print("   ‚úì models/baseline_models.pkl")
print("   ‚úì models/model_comparison.json")
print("   ‚úì models/hyperparameter_tuning.json")
print("   ‚úì models/feature_importance.json")
print("   ‚úì reports/baseline_comparison.csv")
print("   ‚úì reports/baseline_models_comparison.png")
print("   ‚úì reports/error_analysis.png")
print("   ‚úì reports/feature_importance.png")
print("   ‚úì reports/predictions_vs_reality.png")
print("   ‚úì reports/feature_importance.csv")

print("\n" + "="*70)
print("‚úÖ MODELING TERMIN√â AVEC SUCC√àS!")
print("="*70)
